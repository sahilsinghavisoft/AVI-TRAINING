{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am performing this task with both pandas and pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Walmart Data Analysis Project\n",
    "\n",
    "Let's get some quick practice with your new Spark DataFrame skills, you will be asked some basic questions about some stock market data, in this case Walmart Stock from the years 2012-2017. This exercise will just ask a bunch of questions, unlike the machine learning exercises, which will be a little looser and be in the form of \"Consulting Projects\".\n",
    "\n",
    "For now, just answer the questions and complete the tasks below.\n",
    "\n",
    "Use the walmart_stock.csv file to Answer and complete the tasks below!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Load the Walmart Stock CSV File, have Spark infer the data types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date       Open       High        Low      Close    Volume  Adj Close\n",
      "0  2012-01-03  59.970001  61.060001  59.869999  60.330002  12668800  52.619235\n",
      "1  2012-01-04  60.209999  60.349998  59.470001  59.709999   9593300  52.078475\n",
      "2  2012-01-05  59.349998  59.619999  58.369999  59.419998  12768200  51.825539\n",
      "3  2012-01-06  59.419998  59.450001  58.869999  59.000000   8069400  51.459220\n",
      "4  2012-01-09  59.029999  59.549999  58.919998  59.180000   6679300  51.616215\n",
      "Date          object\n",
      "Open         float64\n",
      "High         float64\n",
      "Low          float64\n",
      "Close        float64\n",
      "Volume         int64\n",
      "Adj Close    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#pandas\n",
    "import pandas as pd\n",
    "stock=pd.read_csv('walmart_stock.csv')\n",
    "print(stock.head(5))\n",
    "print(stock.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Date: date, Open: double, High: double, Low: double, Close: double, Volume: int, Adj Close: double]\n"
     ]
    }
   ],
   "source": [
    "#pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('stock').getOrCreate()\n",
    "spark\n",
    "df_pyspark=spark.read.csv(\"walmart_stock.csv\",inferSchema=True, header=True)\n",
    "print(df_pyspark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.What are the column names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#pandas\n",
    "print(stock.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pyspark\n",
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.What does the Schema look like?\n",
    "A.root\n",
    " |-- Date: timestamp (nullable = true)\n",
    " |-- Open: double (nullable = true)\n",
    " |-- High: double (nullable = true)\n",
    " |-- Low: double (nullable = true)\n",
    " |-- Close: double (nullable = true)\n",
    " |-- Volume: integer (nullable = true)\n",
    " |-- Adj Close: double (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1258 entries, 0 to 1257\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Date       1258 non-null   object \n",
      " 1   Open       1258 non-null   float64\n",
      " 2   High       1258 non-null   float64\n",
      " 3   Low        1258 non-null   float64\n",
      " 4   Close      1258 non-null   float64\n",
      " 5   Volume     1258 non-null   int64  \n",
      " 6   Adj Close  1258 non-null   float64\n",
      "dtypes: float64(5), int64(1), object(1)\n",
      "memory usage: 68.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#pandas\n",
    "print(stock.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pyspark\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Print out the first 5 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date       Open       High        Low      Close\n",
      "0  2012-01-03  59.970001  61.060001  59.869999  60.330002\n",
      "1  2012-01-04  60.209999  60.349998  59.470001  59.709999\n",
      "2  2012-01-05  59.349998  59.619999  58.369999  59.419998\n",
      "3  2012-01-06  59.419998  59.450001  58.869999  59.000000\n",
      "4  2012-01-09  59.029999  59.549999  58.919998  59.180000\n"
     ]
    }
   ],
   "source": [
    "#pandas\n",
    "print(stock.iloc[:, :5].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Date=datetime.date(2012, 1, 3), Open=59.970001, High=61.060001, Low=59.869999, Close=60.330002, Volume=12668800, Adj Close=52.619234999999996) \n",
      "\n",
      "Row(Date=datetime.date(2012, 1, 4), Open=60.209998999999996, High=60.349998, Low=59.470001, Close=59.709998999999996, Volume=9593300, Adj Close=52.078475) \n",
      "\n",
      "Row(Date=datetime.date(2012, 1, 5), Open=59.349998, High=59.619999, Low=58.369999, Close=59.419998, Volume=12768200, Adj Close=51.825539) \n",
      "\n",
      "Row(Date=datetime.date(2012, 1, 6), Open=59.419998, High=59.450001, Low=58.869999, Close=59.0, Volume=8069400, Adj Close=51.45922) \n",
      "\n",
      "Row(Date=datetime.date(2012, 1, 9), Open=59.029999, High=59.549999, Low=58.919998, Close=59.18, Volume=6679300, Adj Close=51.616215000000004) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pyspark\n",
    "for line in df_pyspark.head(5):\n",
    "    print(line,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Use describe() to learn about the DataFrame.\n",
    "A.+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
    "|summary|              Open|             High|              Low|            Close|           Volume|        Adj Close|\n",
    "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
    "|  count|              1258|             1258|             1258|             1258|             1258|             1258|\n",
    "|   mean| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n",
    "| stddev|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|\n",
    "|    min|56.389998999999996|        57.060001|        56.299999|        56.419998|          2094900|        50.363689|\n",
    "|    max|         90.800003|        90.970001|            89.25|        90.470001|         80898100|84.91421600000001|\n",
    "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Open         High          Low        Close        Volume  \\\n",
      "count  1258.000000  1258.000000  1258.000000  1258.000000  1.258000e+03   \n",
      "mean     72.357854    72.839388    71.918601    72.388450  8.222093e+06   \n",
      "std       6.768090     6.768187     6.744076     6.756859  4.519781e+06   \n",
      "min      56.389999    57.060001    56.299999    56.419998  2.094900e+06   \n",
      "25%      68.627503    69.059998    68.162503    68.632497  5.791100e+06   \n",
      "50%      73.235000    73.725002    72.839996    73.265000  7.093500e+06   \n",
      "75%      76.629997    77.094999    76.250000    76.709999  9.394675e+06   \n",
      "max      90.800003    90.970001    89.250000    90.470001  8.089810e+07   \n",
      "\n",
      "         Adj Close  \n",
      "count  1258.000000  \n",
      "mean     67.238838  \n",
      "std       6.722609  \n",
      "min      50.363689  \n",
      "25%      63.778335  \n",
      "50%      68.541162  \n",
      "75%      71.105668  \n",
      "max      84.914216  \n"
     ]
    }
   ],
   "source": [
    "#pandas\n",
    "print(stock.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/08 12:51:12 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|summary|              Open|             High|              Low|            Close|           Volume|        Adj Close|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|  count|              1258|             1258|             1258|             1258|             1258|             1258|\n",
      "|   mean| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n",
      "| stddev|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|\n",
      "|    min|56.389998999999996|        57.060001|        56.299999|        56.419998|          2094900|        50.363689|\n",
      "|    max|         90.800003|        90.970001|            89.25|        90.470001|         80898100|84.91421600000001|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pyspark\n",
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.There are too many decimal places for mean and stddev in the describe() dataframe. Format the numbers to just show up to two decimal places. Pay careful attention to the datatypes that .describe() returns, we didn't cover how to do this exact formatting, but we covered something very similar.\n",
    "\n",
    "A.+-------+--------+--------+--------+--------+----------+\n",
    "|summary|    Open|    High|     Low|   Close|    Volume|\n",
    "+-------+--------+--------+--------+--------+----------+\n",
    "|  count|1,258.00|1,258.00|1,258.00|1,258.00|     1,258|\n",
    "|   mean|   72.36|   72.84|   71.92|   72.39| 8,222,093|\n",
    "| stddev|    6.77|    6.77|    6.74|    6.76| 4,519,780|\n",
    "|    min|   56.39|   57.06|   56.30|   56.42| 2,094,900|\n",
    "|    max|   90.80|   90.97|   89.25|   90.47|80,898,100|\n",
    "+-------+--------+--------+--------+--------+----------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_e6e20\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e6e20_level0_col0\" class=\"col_heading level0 col0\" >Open</th>\n",
       "      <th id=\"T_e6e20_level0_col1\" class=\"col_heading level0 col1\" >High</th>\n",
       "      <th id=\"T_e6e20_level0_col2\" class=\"col_heading level0 col2\" >Low</th>\n",
       "      <th id=\"T_e6e20_level0_col3\" class=\"col_heading level0 col3\" >Close</th>\n",
       "      <th id=\"T_e6e20_level0_col4\" class=\"col_heading level0 col4\" >Volume</th>\n",
       "      <th id=\"T_e6e20_level0_col5\" class=\"col_heading level0 col5\" >Adj Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e20_level0_row0\" class=\"row_heading level0 row0\" >count</th>\n",
       "      <td id=\"T_e6e20_row0_col0\" class=\"data row0 col0\" >1,258.00</td>\n",
       "      <td id=\"T_e6e20_row0_col1\" class=\"data row0 col1\" >1,258.00</td>\n",
       "      <td id=\"T_e6e20_row0_col2\" class=\"data row0 col2\" >1,258.00</td>\n",
       "      <td id=\"T_e6e20_row0_col3\" class=\"data row0 col3\" >1,258.00</td>\n",
       "      <td id=\"T_e6e20_row0_col4\" class=\"data row0 col4\" >1,258</td>\n",
       "      <td id=\"T_e6e20_row0_col5\" class=\"data row0 col5\" >1258.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e20_level0_row1\" class=\"row_heading level0 row1\" >mean</th>\n",
       "      <td id=\"T_e6e20_row1_col0\" class=\"data row1 col0\" >72.36</td>\n",
       "      <td id=\"T_e6e20_row1_col1\" class=\"data row1 col1\" >72.84</td>\n",
       "      <td id=\"T_e6e20_row1_col2\" class=\"data row1 col2\" >71.92</td>\n",
       "      <td id=\"T_e6e20_row1_col3\" class=\"data row1 col3\" >72.39</td>\n",
       "      <td id=\"T_e6e20_row1_col4\" class=\"data row1 col4\" >8,222,093</td>\n",
       "      <td id=\"T_e6e20_row1_col5\" class=\"data row1 col5\" >67.238838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e20_level0_row2\" class=\"row_heading level0 row2\" >std</th>\n",
       "      <td id=\"T_e6e20_row2_col0\" class=\"data row2 col0\" >6.77</td>\n",
       "      <td id=\"T_e6e20_row2_col1\" class=\"data row2 col1\" >6.77</td>\n",
       "      <td id=\"T_e6e20_row2_col2\" class=\"data row2 col2\" >6.74</td>\n",
       "      <td id=\"T_e6e20_row2_col3\" class=\"data row2 col3\" >6.76</td>\n",
       "      <td id=\"T_e6e20_row2_col4\" class=\"data row2 col4\" >4,519,781</td>\n",
       "      <td id=\"T_e6e20_row2_col5\" class=\"data row2 col5\" >6.722609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e20_level0_row3\" class=\"row_heading level0 row3\" >min</th>\n",
       "      <td id=\"T_e6e20_row3_col0\" class=\"data row3 col0\" >56.39</td>\n",
       "      <td id=\"T_e6e20_row3_col1\" class=\"data row3 col1\" >57.06</td>\n",
       "      <td id=\"T_e6e20_row3_col2\" class=\"data row3 col2\" >56.30</td>\n",
       "      <td id=\"T_e6e20_row3_col3\" class=\"data row3 col3\" >56.42</td>\n",
       "      <td id=\"T_e6e20_row3_col4\" class=\"data row3 col4\" >2,094,900</td>\n",
       "      <td id=\"T_e6e20_row3_col5\" class=\"data row3 col5\" >50.363689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e20_level0_row4\" class=\"row_heading level0 row4\" >25%</th>\n",
       "      <td id=\"T_e6e20_row4_col0\" class=\"data row4 col0\" >68.63</td>\n",
       "      <td id=\"T_e6e20_row4_col1\" class=\"data row4 col1\" >69.06</td>\n",
       "      <td id=\"T_e6e20_row4_col2\" class=\"data row4 col2\" >68.16</td>\n",
       "      <td id=\"T_e6e20_row4_col3\" class=\"data row4 col3\" >68.63</td>\n",
       "      <td id=\"T_e6e20_row4_col4\" class=\"data row4 col4\" >5,791,100</td>\n",
       "      <td id=\"T_e6e20_row4_col5\" class=\"data row4 col5\" >63.778335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e20_level0_row5\" class=\"row_heading level0 row5\" >50%</th>\n",
       "      <td id=\"T_e6e20_row5_col0\" class=\"data row5 col0\" >73.24</td>\n",
       "      <td id=\"T_e6e20_row5_col1\" class=\"data row5 col1\" >73.73</td>\n",
       "      <td id=\"T_e6e20_row5_col2\" class=\"data row5 col2\" >72.84</td>\n",
       "      <td id=\"T_e6e20_row5_col3\" class=\"data row5 col3\" >73.26</td>\n",
       "      <td id=\"T_e6e20_row5_col4\" class=\"data row5 col4\" >7,093,500</td>\n",
       "      <td id=\"T_e6e20_row5_col5\" class=\"data row5 col5\" >68.541162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e20_level0_row6\" class=\"row_heading level0 row6\" >75%</th>\n",
       "      <td id=\"T_e6e20_row6_col0\" class=\"data row6 col0\" >76.63</td>\n",
       "      <td id=\"T_e6e20_row6_col1\" class=\"data row6 col1\" >77.09</td>\n",
       "      <td id=\"T_e6e20_row6_col2\" class=\"data row6 col2\" >76.25</td>\n",
       "      <td id=\"T_e6e20_row6_col3\" class=\"data row6 col3\" >76.71</td>\n",
       "      <td id=\"T_e6e20_row6_col4\" class=\"data row6 col4\" >9,394,675</td>\n",
       "      <td id=\"T_e6e20_row6_col5\" class=\"data row6 col5\" >71.105668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e20_level0_row7\" class=\"row_heading level0 row7\" >max</th>\n",
       "      <td id=\"T_e6e20_row7_col0\" class=\"data row7 col0\" >90.80</td>\n",
       "      <td id=\"T_e6e20_row7_col1\" class=\"data row7 col1\" >90.97</td>\n",
       "      <td id=\"T_e6e20_row7_col2\" class=\"data row7 col2\" >89.25</td>\n",
       "      <td id=\"T_e6e20_row7_col3\" class=\"data row7 col3\" >90.47</td>\n",
       "      <td id=\"T_e6e20_row7_col4\" class=\"data row7 col4\" >80,898,100</td>\n",
       "      <td id=\"T_e6e20_row7_col5\" class=\"data row7 col5\" >84.914216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x12bb9e2c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pandas\n",
    "summary_df = stock.describe()\n",
    "# Format the 'Volume' column to include commas as thousands separator\n",
    "summary_df['Volume'] = summary_df['Volume'].apply(lambda x: f'{x:,.0f}')\n",
    "# Define the formatting dictionary\n",
    "format_dict = {'Open': '{:,.2f}','High': '{:,.2f}', 'Low': '{:,.2f}', 'Close': '{:,.2f}'}\n",
    "# Apply formatting to the DataFrame\n",
    "summary_df = summary_df.style.format(format_dict)\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------+--------+--------+----------+\n",
      "|summary|    Open|    High|     Low|   Close|    Volume|\n",
      "+-------+--------+--------+--------+--------+----------+\n",
      "|  count|1,258.00|1,258.00|1,258.00|1,258.00|     1,258|\n",
      "|   mean|   72.36|   72.84|   71.92|   72.39| 8,222,093|\n",
      "| stddev|    6.77|    6.77|    6.74|    6.76| 4,519,780|\n",
      "|    min|   56.39|   57.06|   56.30|   56.42| 2,094,900|\n",
      "|    max|   90.80|   90.97|   89.25|   90.47|80,898,100|\n",
      "+-------+--------+--------+--------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pyspark\n",
    "from pyspark.sql.functions import format_number\n",
    "summary = df_pyspark.describe()\n",
    "# Apply the format_number function to each column and alias them\n",
    "summary_formatted = summary.select(\n",
    "    summary['summary'],\n",
    "    format_number(summary['Open'].cast('float'), 2).alias('Open'),\n",
    "    format_number(summary['High'].cast('float'), 2).alias('High'),\n",
    "    format_number(summary['Low'].cast('float'), 2).alias('Low'),\n",
    "    format_number(summary['Close'].cast('float'), 2).alias('Close'),\n",
    "    format_number(summary['Volume'].cast('int'), 0).alias('Volume')\n",
    ")\n",
    "summary_formatted.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.Create a new dataframe with a column called HV Ratio that is the ratio of the High Price versus volume of stock traded for a day.\n",
    "\n",
    "A.+--------------------+\n",
    "|            HV Ratio|\n",
    "+--------------------+\n",
    "|4.819714653321546E-6|\n",
    "|6.290848613094555E-6|\n",
    "|4.669412994783916E-6|\n",
    "|7.367338463826307E-6|\n",
    "|8.915604778943901E-6|\n",
    "|8.644477436914568E-6|\n",
    "|9.351828421515645E-6|\n",
    "| 8.29141562102703E-6|\n",
    "|7.712212102001476E-6|\n",
    "|7.071764823529412E-6|\n",
    "|1.015495466386981E-5|\n",
    "|6.576354146362592...|\n",
    "| 5.90145296180676E-6|\n",
    "|8.547679455011844E-6|\n",
    "|8.420709512685392E-6|\n",
    "|1.041448341728929...|\n",
    "|8.316075414862431E-6|\n",
    "|9.721183814992126E-6|\n",
    "|8.029436027707578E-6|\n",
    "|6.307432259386365E-6|\n",
    "+--------------------+\n",
    "only showing top 20 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      HV_Ratio\n",
      "0     0.000005\n",
      "1     0.000006\n",
      "2     0.000005\n",
      "3     0.000007\n",
      "4     0.000009\n",
      "...        ...\n",
      "1253  0.000015\n",
      "1254  0.000016\n",
      "1255  0.000014\n",
      "1256  0.000016\n",
      "1257  0.000010\n",
      "\n",
      "[1258 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#pandas\n",
    "hv_ratio_df = stock.assign(HV_Ratio=stock['High'] / stock['Volume'])\n",
    "print(hv_ratio_df[['HV_Ratio']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            HV Ratio|\n",
      "+--------------------+\n",
      "|4.819714653321546E-6|\n",
      "|6.290848613094555E-6|\n",
      "|4.669412994783916E-6|\n",
      "|7.367338463826307E-6|\n",
      "|8.915604778943901E-6|\n",
      "|8.644477436914568E-6|\n",
      "|9.351828421515645E-6|\n",
      "| 8.29141562102703E-6|\n",
      "|7.712212102001476E-6|\n",
      "|7.071764823529412E-6|\n",
      "|1.015495466386981E-5|\n",
      "|6.576354146362592...|\n",
      "| 5.90145296180676E-6|\n",
      "|8.547679455011844E-6|\n",
      "|8.420709512685392E-6|\n",
      "|1.041448341728929...|\n",
      "|8.316075414862431E-6|\n",
      "|9.721183814992126E-6|\n",
      "|8.029436027707578E-6|\n",
      "|6.307432259386365E-6|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pyspark\n",
    "df2 = df_pyspark.withColumn(\"HV Ratio\",df_pyspark[\"High\"]/df_pyspark[\"Volume\"])\n",
    "df2.select('HV Ratio').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.What day had the Peak High in Price?\n",
    "A.datetime.datetime(2015, 1, 13, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-13\n"
     ]
    }
   ],
   "source": [
    "#pandas\n",
    "peak_high_row = stock[stock['High'] == stock['High'].max()]\n",
    "peak_high_date = peak_high_row['Date'].iloc[0]\n",
    "print(peak_high_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2015, 1, 13)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pyspark\n",
    "df_pyspark.orderBy(df_pyspark[\"High\"].desc()).head(1)[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.What is the max and min of the Volume column?\n",
    "A.+-----------+-----------+\n",
    "|max(Volume)|min(Volume)|\n",
    "+-----------+-----------+\n",
    "|   80898100|    2094900|\n",
    "+-----------+-----------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2094900\n",
      "80898100\n"
     ]
    }
   ],
   "source": [
    "#pandas\n",
    "print(min(stock.Volume))\n",
    "print(max(stock.Volume))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|max(Volume)|min(Volume)|\n",
      "+-----------+-----------+\n",
      "|   80898100|    2094900|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pyspark\n",
    "from pyspark.sql.functions import max,min\n",
    "df_pyspark.select(max(\"Volume\"),min(\"Volume\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.How many days was the Close lower than 60 dollars?\n",
    "A.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "#pandas\n",
    "lower_than_60_df = stock[stock['Close'] < 60]\n",
    "days_lower_than_60 = len(lower_than_60_df)\n",
    "print(days_lower_than_60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pyspark\n",
    "df_pyspark.filter(df_pyspark['Close'] < 60).count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.What percentage of the time was the High greater than 80 dollars ?\n",
    "A. 9.141494"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.141494\n"
     ]
    }
   ],
   "source": [
    "#pandas\n",
    "totaldays = len(stock)\n",
    "high_greater_than_80_days = stock[stock['High'] > 80]\n",
    "num_high_greater_than_80_days = len(high_greater_than_80_days)\n",
    "percentage_high_greater_than_80 = (num_high_greater_than_80_days / totaldays) * 100\n",
    "print(round(percentage_high_greater_than_80, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.141494435612083"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pyspark\n",
    "df_pyspark.filter('High > 80').count() * 100/df_pyspark.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.What is the Pearson correlation between High and Volume?\n",
    "A.+-------------------+\n",
    "| corr(High, Volume)|\n",
    "+-------------------+\n",
    "|-0.3384326061737161|\n",
    "+-------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3384326061737166\n"
     ]
    }
   ],
   "source": [
    "#pandas\n",
    "pearson_corr = stock['High'].corr(stock['Volume'])\n",
    "print(pearson_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3384326061737161"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pyspark\n",
    "df_pyspark.corr(\"High\",\"Volume\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.What is the max High per year?\n",
    "A.+----+---------+\n",
    "|Year|max(High)|\n",
    "+----+---------+\n",
    "|2015|90.970001|\n",
    "|2013|81.370003|\n",
    "|2014|88.089996|\n",
    "|2012|77.599998|\n",
    "|2016|75.190002|\n",
    "+----+---------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year       High\n",
      "0  2012  77.599998\n",
      "1  2013  81.370003\n",
      "2  2014  88.089996\n",
      "3  2015  90.970001\n",
      "4  2016  75.190002\n"
     ]
    }
   ],
   "source": [
    "#pandas\n",
    "stock['Date'] = pd.to_datetime(stock['Date'])\n",
    "stock['Year'] = stock['Date'].dt.year\n",
    "# group by 'Year' and find the maximum high for each year\n",
    "max_high_per_year = stock.groupby('Year')['High'].max().reset_index()\n",
    "\n",
    "print(max_high_per_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|Year|max(High)|\n",
      "+----+---------+\n",
      "|2015|90.970001|\n",
      "|2013|81.370003|\n",
      "|2014|88.089996|\n",
      "|2012|77.599998|\n",
      "|2016|75.190002|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Pyspark\n",
    "from pyspark.sql.functions import year\n",
    "yeardf = df_pyspark.withColumn(\"Year\",year(df_pyspark[\"Date\"]))\n",
    "max_df = yeardf.groupBy('Year').max()\n",
    "max_df.select('Year','max(High)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14.What is the average Close for each Calendar Month?\n",
    "In other words, across all the years, what is the average Close price for Jan,Feb, Mar, etc... Your result will have a value for each of these months.\n",
    "\n",
    "A.+-----+-----------------+\n",
    "|Month|       avg(Close)|\n",
    "+-----+-----------------+\n",
    "|    1|71.44801958415842|\n",
    "|    2|  71.306804443299|\n",
    "|    3|71.77794377570092|\n",
    "|    4|72.97361900952382|\n",
    "|    5|72.30971688679247|\n",
    "|    6| 72.4953774245283|\n",
    "|    7|74.43971943925233|\n",
    "|    8|73.02981855454546|\n",
    "|    9|72.18411785294116|\n",
    "|   10|71.57854545454543|\n",
    "|   11| 72.1110893069307|\n",
    "|   12|72.84792478301885|\n",
    "+-----+-----------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Month      Close\n",
      "0       1  71.448020\n",
      "1       2  71.306804\n",
      "2       3  71.777944\n",
      "3       4  72.973619\n",
      "4       5  72.309717\n",
      "5       6  72.495377\n",
      "6       7  74.439719\n",
      "7       8  73.029819\n",
      "8       9  72.184118\n",
      "9      10  71.578545\n",
      "10     11  72.111089\n",
      "11     12  72.847925\n"
     ]
    }
   ],
   "source": [
    "#pandas\n",
    "stock['Date'] = pd.to_datetime(stock['Date'])\n",
    "stock['Month'] = stock['Date'].dt.month\n",
    "#group by 'Month' and find the average close for each month\n",
    "avg_close_per_month = stock.groupby('Month')['Close'].mean().reset_index()\n",
    "\n",
    "print(avg_close_per_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|     Month|        avg(Close)|\n",
      "+----------+------------------+\n",
      "|2012-01-03|         60.330002|\n",
      "|2012-01-04|59.709998999999996|\n",
      "|2012-01-05|         59.419998|\n",
      "|2012-01-06|              59.0|\n",
      "|2012-01-09|             59.18|\n",
      "|2012-01-10|59.040001000000004|\n",
      "|2012-01-11|         59.400002|\n",
      "|2012-01-12|              59.5|\n",
      "|2012-01-13|59.540001000000004|\n",
      "|2012-01-17|         59.849998|\n",
      "|2012-01-18|60.009997999999996|\n",
      "|2012-01-19|60.610001000000004|\n",
      "|2012-01-20|61.009997999999996|\n",
      "|2012-01-23|             60.91|\n",
      "|2012-01-24|61.389998999999996|\n",
      "|2012-01-25|         61.470001|\n",
      "|2012-01-26|         60.970001|\n",
      "|2012-01-27|60.709998999999996|\n",
      "|2012-01-30|         61.299999|\n",
      "|2012-01-31|61.360001000000004|\n",
      "+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pyspark\n",
    "month_df = df_pyspark.withColumn('Month',(df_pyspark['Date']))\n",
    "#group by month and take average of all other columns\n",
    "month_df = month_df.groupBy('Month').mean()\n",
    "#sort by month\n",
    "month_df = month_df.orderBy('Month')\n",
    "month_df['Month', 'avg(Close)'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
